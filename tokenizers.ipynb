{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringIO\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Upload the processed CSV file\u001b[39;00m\n\u001b[0;32m      8\u001b[0m uploaded \u001b[38;5;241m=\u001b[39m files\u001b[38;5;241m.\u001b[39mupload()  \u001b[38;5;66;03m# Upload your file\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pycparser import c_parser, c_ast\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from google.colab import files\n",
    "\n",
    "# Upload the processed CSV file\n",
    "uploaded = files.upload()  # Upload your file\n",
    "\n",
    "# Load dataset\n",
    "file_path = list(uploaded.keys())[0]  # Get the uploaded filename dynamically\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def tokenize_code(code):\n",
    "    if pd.isna(code) or not isinstance(code, str):\n",
    "        return []\n",
    "\n",
    "    # Remove comments\n",
    "    code = re.sub(r'//.*?|/\\*.*?\\*/', '', code, flags=re.S).strip()\n",
    "\n",
    "    # Ensure it's a valid C snippet by wrapping it in a basic function\n",
    "    wrapped_code = f\"void dummy(){{\\n{code}\\n}}\"\n",
    "\n",
    "    parser = c_parser.CParser()\n",
    "    try:\n",
    "        ast = parser.parse(wrapped_code)\n",
    "        return extract_tokens(ast)\n",
    "    except Exception:\n",
    "        # Fallback to basic tokenization using regex\n",
    "        return re.findall(r'\\w+|\\S', code)\n",
    "\n",
    "def extract_tokens(ast):\n",
    "    tokens = []\n",
    "\n",
    "    class TokenVisitor(c_ast.NodeVisitor):\n",
    "        def __init__(self):\n",
    "            self.tokens = []\n",
    "\n",
    "        def visit(self, node):\n",
    "            if isinstance(node, c_ast.ID):\n",
    "                self.tokens.append(node.name)\n",
    "            self.generic_visit(node)\n",
    "\n",
    "    visitor = TokenVisitor()\n",
    "    visitor.visit(ast)\n",
    "\n",
    "    return visitor.tokens\n",
    "\n",
    "# Apply tokenization to dataset\n",
    "df['Tokens'] = df['Faulty Code'].apply(tokenize_code)\n",
    "\n",
    "# Save the processed DataFrame to a CSV file\n",
    "output_filename = \"tokenized_dataset.csv\"\n",
    "df.to_csv(output_filename, index=False)\n",
    "\n",
    "# Download the file\n",
    "files.download(output_filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
